# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FINWIqXwjdbra7jT1LHjb2bzR2sjnhx9

****Penerapan Algoritma _Clustering_ untuk Pengelompokkan Saham IDX Berdasarkan Indikator-indikator Fundamental - Submission Machine Learning Terapan Dicoding****

oleh: Fikri Septrian Anggara (fikri_anggara_2c3r)

### Import library yang diperlukan
"""

# pengolahan data
import pandas as pd
import numpy as np

# visualisasi data
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

# pembangunan klaster
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN

# Tuning
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import NearestNeighbors

# scaling data
from sklearn.preprocessing import MinMaxScaler

# imputasi data
from sklearn.impute import SimpleImputer

# reduksi dimensi
from sklearn.decomposition import PCA

# evaluasi klaster
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# disable warning
import warnings
warnings.simplefilter(action='ignore')

"""# 2. Data Understanding

### 2.1. Menyiapkan Dataset
dataset diperoleh dari kaggle, terdapat dua dataset yang digunakan yaitu:
 - [financial statement idx stocks](https://www.kaggle.com/datasets/greegtitan/financial-statement-idx-stocks?resource=download) (kaggle). terakhir diupdate pada Oktober 2022.
 - [daftar saham](https://www.kaggle.com/datasets/muamkh/ihsgstockdata?select=DaftarSaham.csv) (kaggle). terakhir diupdate pada Januari 2023
"""

# load data
stockQuarter = pd.read_csv('../data/quarter.csv')
masterStock = pd.read_csv('../data/DaftarSaham.csv')

"""### 2.2. Overview Data
pada tahap ini dilakukan _overview_ pada data stockQuarter dan masterStock
"""

# banyaknya baris dan kolom (baris, kolom)
print(stockQuarter.shape)
print(masterStock.shape)

# kolom dan tipe data
stockQuarter.info()

# kolom dan tipe data
masterStock.info()

# ambil sampel random sebanyak 5 data
stockQuarter.sample(n=5, random_state=1)

stockQuarter.isnull().sum()

"""- terdapat banyak ****null value**** pada data saham perkuarter"""

# ambil sampel random sebanyak 5 data
masterStock.sample(n=5, random_state=1)

# 5 number summary stock quarter
stockQuarter.describe(include='all').T

"""- terdapat **544** buah saham
- terdapat **388** variabel pada laporan keuangan
- terdapat **3** kategori akun yaitu balance sheet, cash-flow dan income statement
"""

# 5 number summary master stock
masterStock.describe(include='all').T

"""- Terdapat 11 sektor pada master stok dengan **Consumer Cyclicals** adalah sektor yang paling banyak emitennya
- Terdapat 829 buah emiten
"""

# variabel pada laporan keuangan
pd.DataFrame(pd.unique(stockQuarter['type']))

"""_dataframe_ **stockQuarter** memiliki 208691 baris dan 8 kolom. ke delapan kolom yaitu :
 - **symbol**: Kode saham IDX seperti BBRI, BBCA, BMRI, dst.
 - **account**: Akun laporan keuangan. nilainya meliputi **BS** untuk _Balance Sheet_, **IS** untuk akun _Income Statement_, dan **CF** untuk akun _Cash Flow_.
 - **type**: Tipe/variabel laporan keuangan seperti data total aset, kredit, dividen yang dibayarkan, dst. memiliki 388 tipe.
 - Kolom data variabel laporan keuangan perkuarter. meliputi tanggal **2021-09-30**, **2021-12-31**, **2022-03-31**, **2022-06-30**, **2022-09-30**.

_dataframe_ **masterStock** memiliki 829 baris dan 14 kolom. ke empat belas kolom tersebut yaitu:
 - **Code**: Kode saham IDX
 - **Name**: Nama saham
 - **ListingDate**: Tanggal pendaftaran saham
 - **Shares**: Total saham beredar
 - **ListingBoard**: Tingkat pasar saham, meliputi tingkat akselerasi, pengembangan dan utama
 - **Sector**: Sektor perusahaan
 - **LastPrice**: Harga terakhir saham
 - **MarketCap**: Total nilai perusahaan
 - **MinutesFirstAdded**: Menit pertama data ditambahkan
 - **MinutesLastUpdated**: Menit terakhir data diperbarui
 - **HourlyFirstAdded**: Jam pertama data ditambahkan
 - **HourlyLastUpdated**: Jam terakhir data diperbarui
 - **DailyFirstAdded**: Tanggal Pertama data ditambahkan
 - **DailyLastUpdated**: Tanggal Terakhir data diperbarui

terdapat 389 variabel pada laporan keuangan

Berdasarkan _Overview Data_, diketahui :
- Terdapat banyak **null value** pada data saham perkuarter
- Terdapat **544** buah saham yang tercatat laporan keuangannya
- Terdapat **388** variabel pada laporan keuangan
- Terdapat **3** kategori akun yaitu balance sheet, cash-flow dan income statement
- Terdapat **11** sektor pada master stok dengan **Consumer Cyclicals** adalah sektor yang paling banyak emitennya
- Terdapat total **829** buah saham
- Terdapat **389** variabel pada laporan keuangan

# 3. Data Preparation

Data stockQuarter masih belum memiliki struktur yang bisa digunakan untuk pembuatan model dan belum digabung dengan masterStock untuk memperoleh data sektor.
Maka pada ada tahap _Data Preparation_, penulis :
1. merubah struktur data stockQuarter agar cocok untuk pembangunan model klaster
2. menggabungkan data stockQuarter dan masterStock
3. melakukan feature engineering untuk memperoleh indikator fundamental yang digunakan pada paper Examining the effectiveness of fundamental analysis
in a long-term stock portfolio
4. melakukan feature selection
5. melakukan analisis data eksploratif
6. melakukan imputasi pada fitur
7. melakukan scaling

karena data stockQuarter terbaru (2022-09-30) memiliki paling banyak _null value_, maka penulis menggunakan data kuarter sebelumnya, yaitu data kuarter kedua tahun 2022.
"""

# buat kolom baru, gabungan antara account dan type untuk menyimpan nilai tipe akun
stockQuarter['account_type'] = stockQuarter['account']+'_'+stockQuarter['type']
stockQuarter.head()

"""### 3.1 Mengubah Struktur Data"""

# ambil data 2022 kuarter dua saja
data2022 = stockQuarter[['symbol', 'account_type', '2022-03-31']].copy()

# reshaping data, kolom" data yang baru merupakan value dari 'type' dari df yang lama
dataReshaped = pd.DataFrame(data2022.pivot_table(
    index='symbol',
    columns='account_type',
    values='2022-03-31'
).reset_index())
dataReshaped.sample(n=5, random_state=1)

# saham yang tersedia
pd.DataFrame(pd.unique(dataReshaped['symbol']))

# cek null value dari masing masing kolom
nulltable = pd.DataFrame(dataReshaped.isnull().sum().reset_index().iloc[1:])
nulltable.columns = ['financial statement', 'sum of null']
print(nulltable)
print('total null: ', dataReshaped.isnull().sum().sum())

# 5 data teratas
dataReshaped.head()

"""### 3.2. Menggabungkan Data"""

# filter kolom master data
filteredMasterStock = masterStock[['Code', 'Name', 'Shares', 'Sector', 'LastPrice']]

# ubah nama kolom agar bisa dijoin
renamedData = dataReshaped.rename(columns={'symbol':'Code'})

# join data stockQuarter dan masterStock
joinedData = pd.merge(left=renamedData, right=filteredMasterStock, on='Code', how='left')

joinedData.sample(5)

"""### 3.3. _Feature Engineering_ indikator fundamental
berdasarkan paper [[5]](fdsafdsavfdsa), terdapat 5 indikator keuangan yang mampu mewakili indikator lain, yaitu:
- **Net Profit Margin**: perbandingan antara net profit/income dengan total revenue. Melihat apakah pengelolaan perusahaan menghasilkan cukup laba dan apakah biaya operasional dan apakah terdapat biaya yang berlebihan.
- **Debt to equity ratio (D/E)** : perbandingan antara total kewajiban (_liabilities_) dengan ekuitas pemegang saham (_shareholder equity_)
- **Current Ratio**: perbandingan antara aset yang dimiliki dengan kewajiban. menunjukkan kemampuan perusahaan melunasi utang jangka pendek dengan aset lancarnya.
- **Earning per share (EPS)**: perbandingan laba/profit (net income) setelah dikurangi pajak dengan jumlah saham yang beredar (outstanding shares). Digunakan untuk melihat profitabilitas perusahaan. Outstanding Shares diperoleh dari Share Issued - Treasury Shares Number.
- **P/E Ratio**: perbandingan antara harga perlembar saham dengan laba tahunan perlembar (EPS). Untuk membandingkan nilai relatif antar perusahaan.
"""

# Net profit margin = net income / total revenue
joinedData['Net_Profit_Margin'] = joinedData['IS_Net Income']/joinedData['IS_Total Revenue']

# debt to equity ratio (D/E)= total debt / stockholder equity
joinedData['Debt_to_Equity_Ratio'] = joinedData['BS_Total Debt']/joinedData['BS_Stockholders Equity']

# Current Ratio = current Assets (cash dan cash equivalents, accounts receivables, Available For Sale Securities)
# / Current liability ( di kasus ini hanya account payable, Current Notes Payable, Income Tax Payable, Trading Liabilities)
joinedData['Curent_Ratio'] = (joinedData['BS_Cash And Cash Equivalents']+joinedData['BS_Accounts Receivable']+joinedData['BS_Available For Sale Securities'])/(joinedData['BS_Accounts Payable'])

# EPS = Net Income / (share issued - treasury shares number)
joinedData['Earning_per_Shares'] = joinedData['IS_Net Income']/(joinedData['BS_Share Issued']-joinedData['BS_Treasury Shares Number'])

# P/E ratio = share price/EPS
joinedData['Price_to_EPS'] = joinedData['LastPrice']/joinedData['Earning_per_Shares']

joinedData['Net_Profit_Margin'].sort_values()

"""### 3.4. Feature Selection
Feature/kolom yang digunakan untuk pemodelan klaster yaitu :
- **Code**
- **Name**
- **Sector**
- **Net Profit Margin**
- **Debt to Equity Ratio**
- **Current Ratio**
- **Earning per Share**
- **Price to EPS**
"""

# filter kolom yang digunakan
usedColumns = ['Code', 'Name', 'Sector', 'Net_Profit_Margin', 'Debt_to_Equity_Ratio', 'Curent_Ratio',
       'Earning_per_Shares', 'Price_to_EPS']
finalData = joinedData[usedColumns]

finalData.sample(5)

"""### 3.5. Exploratory Data Analysis (EDA)

"""

# copy data untuk menghindari perubahan yang tidak diinginkan
edaData = finalData.copy()

# plot proporsi null value
sns.displot(
    data=edaData.isna().melt(value_name="null value"),
    y="variable",
    hue="null value",
    multiple="fill",
    aspect=1.25
)

# cek null value dari masing masing kolom
nulltable = pd.DataFrame(edaData.isnull().sum().reset_index().iloc[1:])
nulltable.columns = ['financial statement', 'sum of null']
nulltable['% null'] = 100*(nulltable['sum of null']/len(edaData)).round(4)

print('total null: ', dataReshaped.isnull().sum().sum())
print(nulltable)

# periksa nilai infinit dan -infinit
edaData.isin([np.inf, -np.inf]).sum()

# fungsi untuk membuat boxplot dan histogram pada satu pallete untuk melihat distribusi data

def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    # buat dua baris plot
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # jumlah baris plot
        sharex=True,
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )
    # boxplot
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )
    # histogram
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # menambahkan mean pada histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # menambahkan median pada histogram

# filter hanya kolom bertipe numerik
numCols = edaData.select_dtypes(include=np.number).columns.tolist()

for col in numCols:
    histogram_boxplot(edaData, col, bins=50, kde=True, figsize=(10, 5))

# fungsi untuk membuat barplot dengan label

def labeled_barplot(data, feature, perc=False, n=None):

    total = len(data[feature])
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # persentase di setiap kategori
        else:
            label = p.get_height()  # jumlah tiap kategori

        x = p.get_x() + p.get_width() / 2
        y = p.get_height()

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )

    plt.show()

labeled_barplot(edaData, "Sector", perc=True)

# plot NPM vs sektor
plt.figure(figsize=(20,8))
sns.boxplot(data = edaData, y = numCols[0], x = "Sector")

# plot Debt to Equity vs sektor
plt.figure(figsize=(20,8))
sns.boxplot(data = edaData, y = numCols[1], x = "Sector")

# plot Current Ratio vs sektor
plt.figure(figsize=(20,8))
sns.boxplot(data = edaData, y = numCols[2], x = "Sector")

# plot EPS vs sektor
plt.figure(figsize=(20,8))
sns.boxplot(data = edaData, y = numCols[3], x = "Sector")

# plot P\E vs sektor
plt.figure(figsize=(20,8))
sns.boxplot(data = edaData, y = numCols[4], x = "Sector")

# melihat korelasi

plt.figure(figsize=(15, 7))
sns.heatmap(
    edaData[numCols].corr(), annot=True, vmin=-1, vmax=1
)
plt.show()

"""### 3.6. Imputasi Data
Imputasi menggunakan median.

"""

imputedData = finalData.copy()

# ganti nilai infinit dan -infinit dengan NaN
imputedData['Net_Profit_Margin'] = imputedData['Net_Profit_Margin'].replace([np.inf, -np.inf], np.nan)

# imputasi data
imputer = SimpleImputer(missing_values=np.nan, strategy='median')
for col in numCols:
    newColName = 'Imputed_'+col
    imputedData[newColName] = imputer.fit_transform(imputedData[col].values.reshape(-1, 1))

imputedData.isnull().sum()

"""### 3.7. scaling data"""

scaledData = imputedData.copy()

scaler = MinMaxScaler()

# ambil hanya kolom bertipe numerik
imputedNumCols = imputedData.select_dtypes(include=np.number).columns.tolist()

for col in imputedNumCols:
    newColName = 'Scaled_'+col
    scaledData[newColName] = scaler.fit_transform(imputedData[col].values.reshape(-1, 1))

scaledData.head()

"""# 4. modelling
pada tahap ini dilakukan pembangunan cluster dengan beberapa algoritma, yaitu:
- K-means
- GMM
- DBSCAN
"""

# persiapkan data untuk masing masing skenario
numFinalCols = scaledData.select_dtypes(include=np.number).columns.tolist()
kMeansData = scaledData[[col for col in numFinalCols if 'Scaled_Imputed' in col]]
gmmData = kMeansData.copy()
dbscanData = kMeansData.copy()

"""### 4.1. Model tanpa tuning

#### 4.1.1. Kmeans
"""

# pembangunan klaster dengan algoritma K-Means sebanyak 5 klaster
kmeans = KMeans(n_clusters=5)
kmeans.fit(kMeansData)
kmeansLabels = kmeans.labels_
kMeansData['K_means_segments'] = kmeansLabels

# reduksi dimensi data menjadi dua dimensi untuk keperluan visualisasi hasil klasterisasi
pca = PCA(n_components=2)

# transforming data and storing results in a dataframe
XReducedPCA = pca.fit_transform(kMeansData)
reducedDfPCA = pd.DataFrame(
    data=XReducedPCA, columns=["Component 1", "Component 2"]
)

sns.scatterplot(
    data=reducedDfPCA,
    x="Component 1",
    y="Component 2",
    hue=kMeansData["K_means_segments"],
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

"""### 4.1.2 Gaussian Mixture Models"""

# GMM

gmmModel = GaussianMixture()
gmmModel.fit(gmmData)
yhat = gmmModel.predict(gmmData)
gmmLabels = yhat
gmmData['GMM_segments'] = yhat

# transforming data and storing results in a dataframe

XReducedPCA = pca.fit_transform(gmmData)
reducedDfPCA = pd.DataFrame(
    data=XReducedPCA, columns=["Component 1", "Component 2"]
)

sns.scatterplot(
    data=reducedDfPCA,
    x="Component 1",
    y="Component 2",
    hue=gmmData["GMM_segments"],
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

"""#### 4.1.3. DBSCAN"""

# DBSCAN

dbscanModel = DBSCAN()
dbscanModel.fit(dbscanData)
yhat = dbscanModel.fit_predict(dbscanData)
dbscanLabels = yhat
dbscanData['DBSCAN_segments'] = yhat

XReducedPCA = pca.fit_transform(dbscanData)
reducedDfPCA = pd.DataFrame(
    data=XReducedPCA, columns=["Component 1", "Component 2"]
)
print('explained variance: ', pca.explained_variance_.sum())
sns.scatterplot(
    data=reducedDfPCA,
    x="Component 1",
    y="Component 2",
    hue=dbscanData['DBSCAN_segments'] ,
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

alg = ['KMeans', 'GMM', 'DBSCAN']
silhouetteScores = [silhouette_score(kMeansData, kmeansLabels), silhouette_score(gmmData, gmmLabels), silhouette_score(dbscanData, dbscanLabels)]
calinskiHarabaszIndex = [calinski_harabasz_score(kMeansData, kmeansLabels), calinski_harabasz_score(gmmData, gmmLabels), calinski_harabasz_score(dbscanData, dbscanLabels)]
DaviesBouldinIndex= [davies_bouldin_score(kMeansData, kmeansLabels), davies_bouldin_score(gmmData, gmmLabels), davies_bouldin_score(dbscanData, dbscanLabels)]

evaluationDf = pd.DataFrame({
    "algoritma":alg,
    "Silhouette Score":silhouetteScores,
    "Calinski-Harabasz Index":calinskiHarabaszIndex,
    "Davies-Bouldin Index":DaviesBouldinIndex
})
evaluationDf

"""### 4.2. Hyperparameter Tuning

#### 4.2.1 Tuning K-Means
"""

# mencari jumlah klaster terbaik dengan nilai silhouette terbaik
sil_score = []
# jumlah klaster dari 2 hingga 14
kMeansData = kMeansData.loc[:, kMeansData.columns != 'K_means_segments']
cluster_list = list(range(2,15))
for n_clusters in cluster_list:
    clusterer = KMeans(n_clusters=n_clusters)
    preds = clusterer.fit_predict((kMeansData))
    score = silhouette_score(kMeansData, preds)
    sil_score.append(score)
    print("untuk jumlah klaster = {}, skor silhouette {})".format(n_clusters, score))

plt.plot(cluster_list,sil_score)
plt.xlabel('jumlah klaster')
plt.ylabel('skor silhouette')
plt.grid()

"""berdasarkan skor silhouette, jumlah klaster terbaik ialah 5

#### 4.2.2. Tuning GMM

Dilakukan dengan melihat skor Bayes Information Criterion, yang dipilih ialah skor BIC terendah.
"""

gmmData = gmmData.loc[:, gmmData.columns !='GMM_segments']

def gmm_bic_score(estimator, X):
    """Callable to pass to GridSearchCV that will use the BIC score."""
    # Make it negative since GridSearchCV expects a score to maximize
    return -estimator.bic(X)


param_grid = {
    "n_components": range(1, 15),
    "covariance_type": ["spherical", "tied", "diag", "full"],
}
grid_search = GridSearchCV(
    GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score
)
grid_search.fit(gmmData)

df = pd.DataFrame(grid_search.cv_results_)[
    ["param_n_components", "param_covariance_type", "mean_test_score"]
]
df["mean_test_score"] = -df["mean_test_score"]
df = df.rename(
    columns={
        "param_n_components": "Number of components",
        "param_covariance_type": "Type of covariance",
        "mean_test_score": "BIC score",
    }
)
df.sort_values(by="BIC score").head()

"""banyak komponen/klaster terbaik ialah 7 komponen dengan tipe kovarians spherical"""

sns.catplot(
    data=df,
    kind="bar",
    x="Number of components",
    y="BIC score",
    hue="Type of covariance",
)
plt.show()

"""#### 4.2.3. Tuning DBSCAN

Terdapat dua parameter yang perlu ditune di algoritma DBSCAN, yaitu epsilon dan sampel minimum.
menurut Sander et al., 1998, sampel minimum yang paling optimal ialah 2 kali dimensi data. kita memiliki 5 feature, sehingga sampel minimum optimal yaitu 10 sampel.
sementara epsilon terbaik bisa dilihat dari grafik elbow dari epsilon vs jarak antar data.
"""

# drop kolom klaster sebelumnya
dbscanData = dbscanData.loc[:, dbscanData.columns !='DBSCAN_segments']
minSample = 2*len(dbscanData.columns)
# hitung jarak menggunakan algoritma nearest neighbors
nn = NearestNeighbors(n_neighbors=minSample).fit(dbscanData)
distances, indices = nn.kneighbors(dbscanData)

distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.figure(figsize=(10,8))
plt.plot(distances)

"""epsilon terbaik berada di sekitar 0.0 hingga 0.2 (daerah yang membentuk siku, paper Determination of Optimal Epsilon (Eps) Value on
DBSCAN Algorithm to Clustering Data on
Peatland Hotspots in Sumatra).
dilakukan perhitungan skor silouette untuk setiap epsilon yang berada di rentang tersebut dengan iterasi epsilon 0.01
"""

# pencarian skor silhouette terbaik

sil_score = []
epsList = np.arange(0.01, 0.2, 0.01)

for eps in epsList:
    clusterer = DBSCAN(eps=eps, min_samples=minSample)
    preds = clusterer.fit_predict((dbscanData))
    score = silhouette_score(dbscanData, preds)
    sil_score.append(score)
    print("untuk epsilon = {}, skor silhouette {})".format(eps, score))

# skor silhouette
plt.plot(epsList,sil_score)
plt.grid()

"""epsilon terbaik ialah yang menghasilkan skor silhouette tertinggi. Terlihat dari grafik bahwa pada epsilon 0.175 dan seterusnya tidak terjadi peningkatan signifikan, sehingga epsilon yang digunakan ialah 0.175

#### 4.3 Modeling dengan Hyperparameter Hasil Tuning
"""

# Tuned Kmeans
kMeansData = kMeansData.loc[:, kMeansData.columns != 'K_means_segments']
kmeansTuned = KMeans(n_clusters=5)
kmeansTuned.fit(kMeansData)
kmeansLabels = kmeansTuned.labels_
kMeansData['K_means_tuned_segments'] = kmeansLabels

XReducedPCA = pca.fit_transform(kMeansData)
reducedDfPCA = pd.DataFrame(
    data=XReducedPCA, columns=["Component 1", "Component 2"]
)
sns.scatterplot(
    data=reducedDfPCA,
    x="Component 1",
    y="Component 2",
    hue=kMeansData['K_means_tuned_segments'] ,
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

# Tuned GMM
gmmData = gmmData.loc[:, gmmData.columns != 'GMM_segments']
gmmTuned = GaussianMixture(n_components=7, covariance_type='spherical')
gmmTuned.fit(gmmData)
gmmLabels = gmmTuned.predict(gmmData)
gmmData['GMM_tuned_segments'] = gmmLabels

XReducedPCA = pca.fit_transform(gmmData)
reducedDfPCA = pd.DataFrame(
    data=XReducedPCA, columns=["Component 1", "Component 2"]
)
sns.scatterplot(
    data=reducedDfPCA,
    x="Component 1",
    y="Component 2",
    hue=gmmData['GMM_tuned_segments'] ,
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

#  Tuned DBSCAN
dbscanData = dbscanData.loc[:, dbscanData.columns !='DBSCAN_segments']
dbscanModel = DBSCAN(eps=0.175, min_samples=minSample)
dbscanModel.fit(dbscanData)
dbscanLabels = dbscanModel.fit_predict(dbscanData)
dbscanData['DBSCAN_tuned_segments'] = dbscanLabels
XReducedPCA = pca.fit_transform(dbscanData)
reducedDfPCA = pd.DataFrame(
    data=XReducedPCA, columns=["Component 1", "Component 2"]
)
sns.scatterplot(
    data=reducedDfPCA,
    x="Component 1",
    y="Component 2",
    hue=dbscanData['DBSCAN_tuned_segments'] ,
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

"""# 5. Evaluasi
evaluasi klaster dilakukan menggunakan :
- Silhouette Score
- Calinski-Harabasz Index
- Davies-Bouldin Index
"""

alg = ['KMeans', 'GMM', 'DBSCAN']
silhouetteScores = [silhouette_score(kMeansData, kmeansLabels), silhouette_score(gmmData, gmmLabels), silhouette_score(dbscanData, dbscanLabels)]
calinskiHarabaszIndex = [calinski_harabasz_score(kMeansData, kmeansLabels), calinski_harabasz_score(gmmData, gmmLabels), calinski_harabasz_score(dbscanData, dbscanLabels)]
DaviesBouldinIndex= [davies_bouldin_score(kMeansData, kmeansLabels), davies_bouldin_score(gmmData, gmmLabels), davies_bouldin_score(dbscanData, dbscanLabels)]

evaluationDf = pd.DataFrame({
    "algoritma":alg,
    "Silhouette Score":silhouetteScores,
    "Calinski-Harabasz Index":calinskiHarabaszIndex,
    "Davies-Bouldin Index":DaviesBouldinIndex
})
evaluationDf

